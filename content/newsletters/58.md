---
title: "Thought Bytes #58"
slug: 58
date: "2020-07-23"
type: learn
excerpt: "An overview of GPT-3: an ELI5 explanation, what it's been used for already, and my opinions on if it's worth the hype."
---
If you've been following tech news recently, you've probably heard a lot about GPT-3. I was fortunate to get access to the Open AI API invite from their CTO Greg Brockman (thanks [@gdb](https://twitter.com/gdb)!) and was able to play around with the model so I wanted to do an overview of GPT-3 from a product perspective.

In today's newsletter, I'll cover the following questions about GPT-3:

- What is GPT-3 (ELI5)?
- What has GPT-3 been used for already?
- Is GPT-3 worth the hype?

### What is GPT-3 (ELI5)?

GPT-3 is a really good language generator, just like you would generate a text reply to a friend‚Äôs text messages. It‚Äôs good at generating any type of text (not just sentences, but also code or translating English to French) as long as you give it a few examples of what to expect.

What makes GPT-3 especially impressive is it doesn‚Äôt take too many examples to figure out what‚Äôs going on. Just like if I told you pairs of words like dog - perro and eat - comer (English - Spanish translation), you‚Äôd know what to say (or lookup) if I said ‚Äúcat‚Äù. Here‚Äôs GPT-3 figuring that out below (GPT-3 output in red):

![openai-api-playground.png](https://kevinarifin.com/newsletters/58/openai-api-playground.png)

From a more technical perspective, GPT-3 is a 175 billion parameter neural network. Those parameters are weights, which are like a fuzzy memory storing data it has seen before - and it has seen a lot. The training corpus is so large that apparently OpenAI has yet to run one epoch over all the training data.

What makes GPT-3 impressive (and practically useful) compared to previous language models is its ability to do no-shot, one-shot, and few-shot learning - it can quickly learn a task from no, one, or few examples. This allows it to perform well on a variety of NLP tasks without having to be fine-tuned for a given task. From the research paper:

> Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art

![/newsletters/58/gpt3-shots.png](https://kevinarifin.com/newsletters/58/gpt3-shots.png)

### What has GPT-3 been used for already?

Hype for GPT-3 has blown up online, especially on Twitter. There have been some really novel ideas that have leveraged GPT-3.

[@sharifshameem](https://twitter.com/sharifshameem) created a description to template code generator. Check out the example of him describing the Google home page and generating the corresponding HTML - amazing. My brother joked that his job is about to get automated.

[![/newsletters/58/html-generation.png](https://kevinarifin.com/newsletters/58/html-generation.png)](https://twitter.com/sharifshameem/status/1283322990625607681)

[@mckaywrigley](https://twitter.com/mckaywrigley) made something that allows you to have conversations with famous people, dead or alive. The site's currently down, but the conversations are amazingly human and sound like something they would actually say.

[![/newsletters/58/learnfromanyone.png](https://kevinarifin.com/newsletters/58/learnfromanyone.png)](https://twitter.com/mckaywrigley/status/1284110063498522624)

This is probably just the tip of the iceberg for things that GPT-3 can do. People are experimenting with GPT-3 to write essays, draft legal paperwork, build a search engine, and censor swear words - some of the things it can do seem straight out of a movie.

### Is GPT-3 worth the hype?

Playing around with GPT-3 myself, I'm amazed. I stayed up late Saturday talking to the AI (Sarah was not happy). It helped me draft part of an income share agreement. I told it I went to Berkeley and it responded that my dream school was Stanford (standfordrejects.com). I ended up implementing a small mentorship bot with GPT-3 and the ease of prompting the model to get useful responses was impressive.

I think the implications of GPT-3 will be huge, just like everyone else on the internet. Some additional thoughts on the future of GPT-3:

- GPT-3 is going to revolutionize applications that involve synthesizing large amounts of information: customer service, knowledge management, and search engines off the top of my head. The ease with which it figures out what you want and synthesizes different pieces of information is so human - you'll probably never see a decision-tree + human response Intercom bot on websites in a few months. In terms of knowledge management, I've always thought an interesting tool in the finance space would be the ability to naturally query financial information like "When was the last time Apple stock dropped more than 5% in a day?".
- I'm curious to see how defensible a GPT-3 based app will be in the future. After playing around with the OpenAI playground a bit, it's pretty easy to recreate prompts that Mckay and Sharif use to generate their code (more on that in two weeks when I do a technical post on creating a Next app with GPT-3). GPT-3 is going to commoditize the ability to process natural language - we'll see how that changes when OpenAI releases their fine-tuning API.
- What gives me most pause is what happens when we rely on GPT-3 for everything. Students could use it to write essays, authors to generate novels, Twitter users to compress long articles into 160 characters, newspapers to write the news. I'm not sure what the takeaway here is, but the idea of a language model generating a majority written content feels dystopian.

To conclude, I saw a tweet that described GPT-3 as apophenia (I had to look up the word as well: "to mistakenly perceive connections and meaning between unrelated things"). The hype may be overblown - we may just be interpolating meaning when there is none to be found. Regardless, this is an impressive human accomplishment - only time will tell if it's just a short fad or an inflection point in human history.

Who knows - maybe this whole newsletter was generated by GPT-3 ü§∑‚Äç‚ôÇÔ∏è.

See you next week ‚úåÔ∏è,

Kevin